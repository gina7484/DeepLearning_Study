{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-1-47e42a38d6bd>, line 40)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-47e42a38d6bd>\"\u001b[1;36m, line \u001b[1;32m40\u001b[0m\n\u001b[1;33m    input img은 X_img이다. (28*28*1 사이즈)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "#출처: hunkim/DeepLearningZeroToAll/lab-11-1-mnist_cnn.py\n",
    "#URL: https://github.com/hunkim/DeepLearningZeroToAll/blob/master/lab-11-1-mnist_cnn.py\n",
    "#Youtube URL: https://www.youtube.com/watch?v=pQ9Y9ZagZBk&feature=youtu.be\n",
    "\n",
    "#직접 돌리기보다는 어려운 CNN만들기 전에 좀 더 단순한 예제 코드보면서 이해하기 위한 용도로 이용\n",
    "\n",
    "# Lab 11 MNIST and Convolutional Neural Network\n",
    "\n",
    "#전체적인 구조\n",
    "# conv - relu - pooling - conv - relu - pooling - fc\n",
    "\n",
    "import tensorflow as tf\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "\n",
    "# hyper parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "X_img = tf.reshape(X, [-1, 28, 28, 1])   # img 28x28x1 (black/white라서 마지막에 1)\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "\n",
    "########## 첫번째 conv layer ##########\n",
    "\n",
    "#1. 사용하려는 필터 크기랑 개수 정해주고 초기화\n",
    "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))  \n",
    "                                 #[3,3,1,32]에서 3*3*1은 필터크기, 32는 필터개수\n",
    "\n",
    "#2. conv layer 어떻게 통과시킬건지 정해주기    \n",
    "L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "#위의 의미:\n",
    "    #input img은 X_img이다. (28*28*1 사이즈)\n",
    "    #W1 필터를 쓸거임.\n",
    "    #stride=[1,1,1,1] 두번째랑 세번째 parameter에 의해 수평,수직으로 각각 한칸씩 이동\n",
    "    #padding='SAME'라서 입력의 이미지 사이즈 28*28과 같아지도록!\n",
    "    #그래서 conv layer 통과시키면    Conv     -> (?, 28, 28, 32)\n",
    "\n",
    "#3. conv layer 출력값에 relu 씌워주기\n",
    "L1 = tf.nn.relu(L1)\n",
    "\n",
    "#4. subsampling(pooling-여기서는 pooling 방법 중 max pooling 이용)\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1],\n",
    "                    strides=[1, 2, 2, 1], padding='SAME')\n",
    "    #ksize=[1,2,2,1] //kernel size=2*2 그래서 2*2의 4칸 중에서 제일 큰 값을 뽑는다\n",
    "    #stride=[1,2,2,1] //2칸씩 수평, 수직으로 이동한다\n",
    "    #padding='SAME' -> 이에 대한 자세한 계산 방법은 https://www.tensorflow.org/api_guides/python/nn#Convolution \n",
    "\n",
    "'''\n",
    "Tensor(\"Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)  //conv layer 통과시켰을 때 결과\n",
    "Tensor(\"Relu:0\", shape=(?, 28, 28, 32), dtype=float32)    //relu 씌워줬을 때 결과 \n",
    "Tensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32) //pooling 이후 결과이자 두번째 conv layer 입력값\n",
    "마지막 결과인 14*14*32를 두번째 conv layer 입력값으로 이용한다\n",
    "'''\n",
    "\n",
    "########## 두 번째 conv layer ##########\n",
    "\n",
    "#1. 사용하려는 필터 크기랑 개수 정해주고 초기화\n",
    "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
    "                                 #[3,3,32,64]에서 3*3*32은 필터크기, 64는 필터개수\n",
    "\n",
    "#2. conv layer 어떻게 통과시킬건지 정해주기\n",
    "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    #    Conv     -> (?, 14, 14, 64)\n",
    "    # padding='SAME'하면 out_height = ceil(float(in_height) / float(strides))니까 14/1해서 14가 된다 -width도 마찬가지\n",
    "\n",
    "#3. conv layer 출력값에 relu 씌워주기\n",
    "L2 = tf.nn.relu(L2)\n",
    "    #    relu     -> (?, 14, 14, 64)\n",
    "\n",
    "#4. subsampling(pooling-여기서는 pooling 방법 중 max pooling 이용)\n",
    "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1],\n",
    "                    strides=[1, 2, 2, 1], padding='SAME')\n",
    "    #    pool     -> (?, 7, 7, 64)\n",
    "    \n",
    "#5. fc에 넣기 위한 reshape 과정\n",
    "L2_flat = tf.reshape(L2, [-1, 7 * 7 * 64])\n",
    "    # -1이 n개의 값이라고 하는데...?\n",
    "\n",
    "'''\n",
    "Tensor(\"Conv2D_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
    "Tensor(\"Relu_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
    "Tensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
    "Tensor(\"Reshape_1:0\", shape=(?, 3136), dtype=float32)\n",
    "'''\n",
    "\n",
    "########## Final FC 7x7x64 inputs -> 10 outputs ##########\n",
    "W3 = tf.get_variable(\"W3\", shape=[7 * 7 * 64, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "logits = tf.matmul(L2_flat, W3) + b\n",
    "\n",
    "########## define cost/loss ##########\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "    # softmax 함수에 대한 자세한 설명: https://bcho.tistory.com/1154\n",
    "    # softmax:  classification 알고리즘중의 하나로, 들어온 값이 어떤 분류인지 구분해주는 알고리즘이다. \n",
    "                #예를 들어 A,B,C 3개의 결과로 분류해주는 소프트맥스의 경우 결과값은 [0.7,0.2,0.1] 와 같이 각각 A,B,C일 확률을 리턴해준다.\n",
    "    # softmax_cross_entropy_with_logits: \n",
    "        # 소프트맥스 함수에 대한 코스트 함수는 크로스엔트로피 (Cross entropy) 함수의 평균을 이용\n",
    "\n",
    "########## define optimizer ##########\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "########## initialize ##########\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "########## train my model ##########\n",
    "print('Learning started. It takes sometime.')\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += (c / total_batch)\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "########## Test model and check accuracy ##########\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "########## Get one and predict ##########\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(logits, 1), feed_dict={X: mnist.test.images[r:r + 1]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
